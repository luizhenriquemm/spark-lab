{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da3dd5ae-aa7c-473d-a403-1f632f7833d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "             .master(\"spark://spark-master:7077\") # Points to the Spark Cluster\n",
    "             .appName('schema-test') # Name the app\n",
    "             .config(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\") # Set external Hive Metastore\n",
    "             .config(\"hive.metastore.warehouse.dir\", \"hdfs://hdfs-namenode:9000/hadoop/warehouse/\") # Set default warehouse dir (legacy) users/hive/warehouse\n",
    "             .config(\"spark.sql.warehouse.dir\", \"hdfs://hdfs-namenode:9000/hadoop/warehouse/\") # Set default warehouse dir\n",
    "             .config(\"hive.metastore.schema.verification\", \"false\") # Prevent some errors\n",
    "             .config(\"fs.defaultFS\", \"hdfs://hdfs-namenode:9000/\") # Set default file system into the HDFS namenode\n",
    "             .enableHiveSupport()\n",
    "             .getOrCreate())\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4bb4e68-5b7a-441b-bfb8-cf0f92e20d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "|Jhon| 35|\n",
      "|Eric| 31|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "    {\"name\": \"Jhon\", \"age\": 35},\n",
    "    {\"name\": \"Eric\", \"age\": 31}\n",
    "], schema=StructType([StructField(\"name\", StringType(), True), StructField(\"age\", IntegerType(), True)]))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc56489e-fb5a-4dce-a2d4-4f01b7c4e6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07d6a06a-cf2b-4491-b033-ca363f355496",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").parquet(\"hdfs://hdfs-namenode:9000/hadoop/warehouse/test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5852f04-f045-4e40-97f8-99149d7f2dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"CREATE EXTERNAL TABLE default.persons (name string, age int) USING PARQUET LOCATION 'hdfs://hdfs-namenode:9000/hadoop/warehouse/persons/'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b044c1a4-3008-4171-b9db-1f1a6ff05974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f87625d4-3366-4e3f-b7fb-cc4578ce9fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|  default|  persons|      false|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af666b8d-c285-4d14-b2c2-28890dcd00e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.insertInto(\"default.persons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "505d5205-679a-4bec-be9f-b861ef1dc0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "|Jhon| 35|\n",
      "|Jhon| 35|\n",
      "|Eric| 31|\n",
      "|Eric| 31|\n",
      "|Jhon| 35|\n",
      "|Eric| 31|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from default.persons\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f40f99fb-36d2-4aec-9ead-72fa0f7eb12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|value                                                                                                                                                                                                    |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[{\"client_id\":49,\"name\":\"a\",\"gender\":\"b\",\"birthdate\":\"2000-01-01\",\"address\":\"c\",\"city\":\"e\",\"state\":\"f\",\"event\":\"create\",\"timestamp\":\"2023-11-13T23:16:41.215105Z\",\"data_criacao\":\"2023-11-13T23:16:42Z\"}]|\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = spark.read.text(\"hdfs://hdfs-namenode:9000/hadoop/warehouse/nifi_test/72287f8a-0765-447c-b10a-ad812cc8535e.json\")\n",
    "a.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b5a6f44-1ed5-4bb0-bf40-a02485f4b12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+------+----------+-------+----+-----+------+---------------------------+\n",
      "|client_id|name|gender|birthdate |address|city|state|event |timestamp                  |\n",
      "+---------+----+------+----------+-------+----+-----+------+---------------------------+\n",
      "|51       |a   |b     |2000-01-01|c      |e   |f    |create|2023-11-13T23:46:04.121484Z|\n",
      "+---------+----+------+----------+-------+----+-----+------+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = spark.read.parquet(\"hdfs://hdfs-namenode:9000/hadoop/warehouse/landing/clients/\")\n",
    "a.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629a9e1a-5c64-4a41-baed-4555bd9618de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

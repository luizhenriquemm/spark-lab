{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c32bc34e-d075-4ea9-84ed-190b06c4a86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "             .master(\"spark://spark-master:7077\") # Points to the Spark Cluster\n",
    "             .appName('lab') # Name the app\n",
    "             .config(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\") # Set external Hive Metastore\n",
    "             .config(\"hive.metastore.warehouse.dir\", \"s3a://minio:9000/datalake/\") # Set default warehouse dir (legacy) users/hive/warehouse\n",
    "             .config(\"spark.sql.warehouse.dir\", \"s3a://minio:9000/datalake/\") # Set default warehouse dir\n",
    "             .config(\"hive.metastore.schema.verification\", \"false\") # Prevent some errors\n",
    "             .config(\"fs.defaultFS\", \"s3a://minio:9000/datalake/\") # Set default file system into the HDFS namenode\n",
    "             .config(\"spark.jars\", \"/opt/bitnami/spark/jars_external/hadoop-aws-3.3.4.jar,/opt/bitnami/spark/jars_external/aws-java-sdk-bundle-1.12.588.jar\")\n",
    "             .config(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "             .enableHiveSupport()\n",
    "             .getOrCreate())\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "hdp_configs = {\n",
    "    \"fs.s3a.endpoint\": \"http://minio:9000\",\n",
    "    \"fs.s3a.access.key\": \"minio\",\n",
    "    \"fs.s3a.secret.key\": \"minioadmin\",\n",
    "    \"fs.s3a.connection.timeout\": \"600000\",\n",
    "    \"spark.sql.debug.maxToStringFields\": \"100\",\n",
    "    \"fs.s3a.path.style.access\": \"true\",\n",
    "    \"fs.s3a.impl\": \"org.apache.hadoop.fs.s3a.S3AFileSystem\",\n",
    "    \"fs.s3a.connection.ssl.enabled\": \"true\"\n",
    "}\n",
    "\n",
    "for k,v in hdp_configs.items():\n",
    "    spark.sparkContext._jsc.hadoopConfiguration().set(k, v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "856034a2-7a01-496b-b8bc-f67fe60076a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "76db62db-3f63-467c-aa58-7bc49f88fe86",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:java.lang.IllegalArgumentException: java.net.UnknownHostException: hdfs-namenode)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdrop database silver cascade\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:1440\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1438\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1439\u001b[0m     litArgs \u001b[38;5;241m=\u001b[39m {k: _to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m {})\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m-> 1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1442\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:java.lang.IllegalArgumentException: java.net.UnknownHostException: hdfs-namenode)"
     ]
    }
   ],
   "source": [
    "spark.sql(\"drop database silver cascade\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3fb8be1e-a569-4a5f-9c48-35481b30d890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>namespace</th></tr>\n",
       "<tr><td>bronze</td></tr>\n",
       "<tr><td>default</td></tr>\n",
       "<tr><td>gold</td></tr>\n",
       "<tr><td>silver</td></tr>\n",
       "<tr><td>source</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---------+\n",
       "|namespace|\n",
       "+---------+\n",
       "|   bronze|\n",
       "|  default|\n",
       "|     gold|\n",
       "|   silver|\n",
       "|   source|\n",
       "+---------+"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"show databases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f99aff0b-9b68-4845-9398-f7b830b0f26e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS source LOCATION 's3a://datalake/source/'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8a8a1190-4730-490d-9373-f7c7a9a5a3f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS gold LOCATION 's3a://datalake/gold/'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "77967e16-4120-4c00-9ba7-8c3d66779a22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS silver LOCATION 's3a://datalake/silver/'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "981b57af-e117-4953-be4f-e5e6994e54dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS bronze LOCATION 's3a://datalake/bronze/'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9aebd4f5-6db6-4767-90bf-c064eae91838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|   source|    final|      false|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables from source\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e0b3256f-d712-4b52-9fb2-8428ce87b732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField('ano_cmpt', StringType(), True), \n",
    "    StructField('mes_cmpt', StringType(), True), \n",
    "    StructField('cgc_hosp', StringType(), True), \n",
    "    StructField('munic_res', StringType(), True), \n",
    "    StructField('nasc', DateType(), True), \n",
    "    StructField('sexo', StringType(), True), \n",
    "    StructField('uti_mes_to', StringType(), True), \n",
    "    StructField('uti_int_to', StringType(), True), \n",
    "    StructField('proc_rea', StringType(), True), \n",
    "    StructField('qt_proc', StringType(), True), \n",
    "    StructField('dt_atend', DateType(), True), \n",
    "    StructField('dt_saida', DateType(), True), \n",
    "    StructField('diag_princ', StringType(), True), \n",
    "    StructField('diag_secun', StringType(), True), \n",
    "    StructField('cobranca', StringType(), True), \n",
    "    StructField('natureza', StringType(), True), \n",
    "    StructField('gestao', StringType(), True), \n",
    "    StructField('munic_mov', StringType(), True), \n",
    "    StructField('cod_idade', StringType(), True), \n",
    "    StructField('idade', StringType(), True), \n",
    "    StructField('dias_perm', StringType(), True), \n",
    "    StructField('morte', StringType(), True), \n",
    "    StructField('cnes', StringType(), True), \n",
    "    StructField('fonte', StringType(), True), \n",
    "    StructField('modalidade', StringType(), True), \n",
    "    StructField('nome_uf', StringType(), True), \n",
    "    StructField('nome_municipio', StringType(), True), \n",
    "    StructField('regiao', StringType(), True), \n",
    "    StructField('idhm', FloatType(), True), \n",
    "    StructField('populacao_residente', IntegerType(), True), \n",
    "    StructField('area_unidade_territorial', FloatType(), True), \n",
    "    StructField('diag_princ_desc', StringType(), True), \n",
    "    StructField('diag_secun_desc', StringType(), True), \n",
    "    StructField('diag_princ_detalhes', StructType([\n",
    "        StructField('sub_cat', StringType(), True), \n",
    "        StructField('classificacao', StringType(), True), \n",
    "        StructField('restr_sexo', StringType(), True), \n",
    "        StructField('causa_obito', StringType(), True), \n",
    "        StructField('descricao', StringType(), True), \n",
    "        StructField('desc_abrev', StringType(), True), \n",
    "        StructField('refer', StringType(), True), \n",
    "        StructField('excluidos', StringType(), True)\n",
    "    ]), True), \n",
    "    StructField('diag_secun_detalhes', StructType([\n",
    "        StructField('sub_cat', StringType(), True), \n",
    "        StructField('classificacao', StringType(), True), \n",
    "        StructField('restr_sexo', StringType(), True), \n",
    "        StructField('causa_obito', StringType(), True), \n",
    "        StructField('descricao', StringType(), True), \n",
    "        StructField('desc_abrev', StringType(), True), \n",
    "        StructField('refer', StringType(), True), \n",
    "        StructField('excluidos', StringType(), True)\n",
    "    ]), True), \n",
    "    StructField('feriado', StringType(), True), \n",
    "    StructField('distancia_feriado', IntegerType(), True), \n",
    "    StructField('feriado_info', ArrayType(StructType([\n",
    "        StructField('data', StringType(), True), \n",
    "        StructField('nome', StringType(), True), \n",
    "        StructField('tipo', StringType(), True), \n",
    "        StructField('descricao', StringType(), True), \n",
    "        StructField('uf', StringType(), True), \n",
    "        StructField('municipio', StringType(), True), \n",
    "        StructField('cod_municipio', StringType(), True)\n",
    "    ]), True), True), \n",
    "    StructField('sigla', StringType(), True)\n",
    "])\n",
    "\n",
    "df_final = spark.createDataFrame([], schema = schema)\n",
    "\n",
    "schema_str = \", \".join([f\"{x[0]} {x[1]}\" for x in df_final.drop(\"sigla\").dtypes ])\n",
    "spark.sql(f\"CREATE EXTERNAL TABLE IF NOT EXISTS source.final ({schema_str}) USING PARQUET PARTITIONED BY (sigla string) LOCATION 's3a://datalake/source/final/'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2359e7f5-3d1d-4512-879b-a4f5b5c1c32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"msck repair table source.final\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d5944db-3668-484f-9ed8-5b63e65269fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "|sigla|   count|\n",
      "+-----+--------+\n",
      "|   SP|67515831|\n",
      "|   RS| 9961350|\n",
      "|   MG| 8143352|\n",
      "|   BA| 5017618|\n",
      "|   SC| 2861516|\n",
      "|   CE|  906485|\n",
      "|   MS|  659648|\n",
      "|   AL|  522891|\n",
      "|   PA|  567822|\n",
      "|   GO|  503891|\n",
      "|   ES|  342183|\n",
      "|   RN|  760980|\n",
      "|   MT|  319959|\n",
      "|   PI|  385786|\n",
      "|   PE|  289395|\n",
      "|   PB|  177462|\n",
      "|   SE|   92552|\n",
      "|   RO|   19372|\n",
      "|   TO|  114201|\n",
      "+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.table(\"source.final\").groupBy(\"sigla\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0bcec92f-4b2a-4b75-bc5d-ce0c492edd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F\n",
    "import builtins as b\n",
    "\n",
    "def StructNormalizer(df):\n",
    "  def SchemaIterator(schema, root=True, prefix=[]):\n",
    "    f = []\n",
    "    for c in schema:\n",
    "      if c.dataType.typeName() == \"struct\":\n",
    "        f += SchemaIterator(schema=c.dataType, root=False, prefix=[*prefix, c.name])\n",
    "      elif not root:\n",
    "        f.append([*prefix, c.name])\n",
    "    return f\n",
    "  nested_cols = SchemaIterator(df.schema)\n",
    "  s = []\n",
    "  for c in df.columns:\n",
    "    if c not in [x[0] for x in nested_cols]:\n",
    "      s.append(F.col(c))\n",
    "    else:\n",
    "      s += [F.col(\".\".join(x)).alias(\"_\".join(x)) for x in b.filter(None, [x if x[0] == c else None for x in nested_cols])]\n",
    "  return df.select(*s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "03d54e8e-6834-4dd2-b429-bcbdd86e8a8f",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o693.saveAsTable.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 11.0 failed 4 times, most recent failure: Lost task 6.3 in stage 11.0 (TID 257) (172.19.0.10 executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Command exited with code 52\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\n\tat org.apache.spark.sql.execution.datasources.DataSource.writeAndRead(DataSource.scala:511)\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.saveDataIntoTable(createDataSourceTables.scala:228)\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:183)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\n\tat org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:697)\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:675)\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:570)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m df \u001b[38;5;241m=\u001b[39m StructNormalizer(spark\u001b[38;5;241m.\u001b[39mtable(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource.final\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m----> 3\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartitionBy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msigla\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgold.final\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:1521\u001b[0m, in \u001b[0;36mDataFrameWriter.saveAsTable\u001b[0;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1520\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[0;32m-> 1521\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o693.saveAsTable.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 11.0 failed 4 times, most recent failure: Lost task 6.3 in stage 11.0 (TID 257) (172.19.0.10 executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Command exited with code 52\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\n\tat org.apache.spark.sql.execution.datasources.DataSource.writeAndRead(DataSource.scala:511)\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.saveDataIntoTable(createDataSourceTables.scala:228)\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:183)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\n\tat org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:697)\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:675)\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:570)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n"
     ]
    }
   ],
   "source": [
    "df = StructNormalizer(spark.table(\"source.final\"))\n",
    "\n",
    "df.write.partitionBy(\"sigla\").mode(\"append\").format(\"parquet\").saveAsTable(\"gold.final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55d14ea7-8e98-4e4c-8949-c3d22788bd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.table(\"source.final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c8987cd-b5e2-4d91-8e12-96910cb882e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>ano_cmpt</th><th>mes_cmpt</th><th>cgc_hosp</th><th>munic_res</th><th>nasc</th><th>sexo</th><th>uti_mes_to</th><th>uti_int_to</th><th>proc_rea</th><th>qt_proc</th><th>dt_atend</th><th>dt_saida</th><th>diag_princ</th><th>diag_secun</th><th>cobranca</th><th>natureza</th><th>gestao</th><th>munic_mov</th><th>cod_idade</th><th>idade</th><th>dias_perm</th><th>morte</th><th>cnes</th><th>fonte</th><th>modalidade</th><th>nome_uf</th><th>nome_municipio</th><th>regiao</th><th>idhm</th><th>populacao_residente</th><th>area_unidade_territorial</th><th>diag_princ_desc</th><th>diag_secun_desc</th><th>diag_princ_detalhes</th><th>diag_secun_detalhes</th><th>feriado</th><th>distancia_feriado</th><th>feriado_info</th><th>sigla</th></tr>\n",
       "<tr><td>2018</td><td>02</td><td>60975174000363</td><td>355030</td><td>1994-07-31</td><td>3</td><td>0</td><td>000</td><td>0301060002</td><td>1</td><td>2018-02-06</td><td>2018-02-06</td><td>Z014</td><td>null</td><td>null</td><td>null</td><td>M</td><td>355030</td><td>4</td><td>23</td><td>0</td><td>0</td><td>2080796</td><td>01</td><td>01</td><td>São Paulo</td><td>São Paulo</td><td>Sudeste</td><td>0.805</td><td>11253503</td><td>1521.101</td><td>Exame ginecológic...</td><td>null</td><td>{Z014, null, F, n...</td><td>{null, null, null...</td><td>false</td><td>6</td><td>[{2018-02-12, Car...</td><td>SP</td></tr>\n",
       "<tr><td>2019</td><td>10</td><td>60979457000707</td><td>353440</td><td>2016-07-15</td><td>3</td><td>0</td><td>000</td><td>0301070121</td><td>1</td><td>2019-10-11</td><td>2019-10-11</td><td>G822</td><td>null</td><td>null</td><td>null</td><td>M</td><td>353440</td><td>4</td><td>03</td><td>0</td><td>0</td><td>5493943</td><td>01</td><td>01</td><td>São Paulo</td><td>Osasco</td><td>Sudeste</td><td>0.776</td><td>666740</td><td>64.954</td><td>Paraplegia não es...</td><td>null</td><td>{G822, null, null...</td><td>{null, null, null...</td><td>false</td><td>1</td><td>[{2019-10-12, Nos...</td><td>SP</td></tr>\n",
       "<tr><td>2018</td><td>02</td><td>60975174000363</td><td>355030</td><td>1959-10-02</td><td>3</td><td>0</td><td>000</td><td>0302050027</td><td>1</td><td>2018-02-06</td><td>2018-02-06</td><td>A300</td><td>null</td><td>null</td><td>null</td><td>M</td><td>355030</td><td>4</td><td>58</td><td>0</td><td>0</td><td>2080796</td><td>01</td><td>01</td><td>São Paulo</td><td>São Paulo</td><td>Sudeste</td><td>0.805</td><td>11253503</td><td>1521.101</td><td>Hanseníase [lepra...</td><td>null</td><td>{A300, null, null...</td><td>{null, null, null...</td><td>false</td><td>6</td><td>[{2018-02-12, Car...</td><td>SP</td></tr>\n",
       "<tr><td>2019</td><td>10</td><td>60979457000707</td><td>353440</td><td>2005-04-01</td><td>3</td><td>0</td><td>000</td><td>0301070121</td><td>1</td><td>2019-10-11</td><td>2019-10-11</td><td>G822</td><td>null</td><td>null</td><td>null</td><td>M</td><td>353440</td><td>4</td><td>14</td><td>0</td><td>0</td><td>5493943</td><td>01</td><td>01</td><td>São Paulo</td><td>Osasco</td><td>Sudeste</td><td>0.776</td><td>666740</td><td>64.954</td><td>Paraplegia não es...</td><td>null</td><td>{G822, null, null...</td><td>{null, null, null...</td><td>false</td><td>1</td><td>[{2019-10-12, Nos...</td><td>SP</td></tr>\n",
       "<tr><td>2018</td><td>02</td><td>60975174000363</td><td>355030</td><td>1997-07-20</td><td>3</td><td>0</td><td>000</td><td>0301060002</td><td>1</td><td>2018-02-06</td><td>2018-02-06</td><td>Z349</td><td>null</td><td>null</td><td>null</td><td>M</td><td>355030</td><td>4</td><td>20</td><td>0</td><td>0</td><td>2080796</td><td>01</td><td>01</td><td>São Paulo</td><td>São Paulo</td><td>Sudeste</td><td>0.805</td><td>11253503</td><td>1521.101</td><td>Supervisão de gra...</td><td>null</td><td>{Z349, null, F, n...</td><td>{null, null, null...</td><td>false</td><td>6</td><td>[{2018-02-12, Car...</td><td>SP</td></tr>\n",
       "<tr><td>2019</td><td>10</td><td>60979457000707</td><td>353440</td><td>2016-07-21</td><td>1</td><td>0</td><td>000</td><td>0301070121</td><td>1</td><td>2019-10-11</td><td>2019-10-11</td><td>G808</td><td>null</td><td>null</td><td>null</td><td>M</td><td>353440</td><td>4</td><td>03</td><td>0</td><td>0</td><td>5493943</td><td>01</td><td>01</td><td>São Paulo</td><td>Osasco</td><td>Sudeste</td><td>0.776</td><td>666740</td><td>64.954</td><td>Outras formas de ...</td><td>null</td><td>{G808, null, null...</td><td>{null, null, null...</td><td>false</td><td>1</td><td>[{2019-10-12, Nos...</td><td>SP</td></tr>\n",
       "<tr><td>2018</td><td>02</td><td>60975174000363</td><td>355030</td><td>1960-12-06</td><td>3</td><td>0</td><td>000</td><td>0301060002</td><td>1</td><td>2018-02-06</td><td>2018-02-06</td><td>K219</td><td>null</td><td>null</td><td>null</td><td>M</td><td>355030</td><td>4</td><td>57</td><td>0</td><td>0</td><td>2080796</td><td>01</td><td>01</td><td>São Paulo</td><td>São Paulo</td><td>Sudeste</td><td>0.805</td><td>11253503</td><td>1521.101</td><td>Doença de refluxo...</td><td>null</td><td>{K219, null, null...</td><td>{null, null, null...</td><td>false</td><td>6</td><td>[{2018-02-12, Car...</td><td>SP</td></tr>\n",
       "<tr><td>2019</td><td>10</td><td>60979457000707</td><td>353440</td><td>1972-02-14</td><td>3</td><td>0</td><td>000</td><td>0302050019</td><td>1</td><td>2019-10-11</td><td>2019-10-11</td><td>M510</td><td>null</td><td>null</td><td>null</td><td>M</td><td>353440</td><td>4</td><td>47</td><td>0</td><td>0</td><td>5493943</td><td>01</td><td>01</td><td>São Paulo</td><td>Osasco</td><td>Sudeste</td><td>0.776</td><td>666740</td><td>64.954</td><td>Transtornos de di...</td><td>null</td><td>{M510, +, null, n...</td><td>{null, null, null...</td><td>false</td><td>1</td><td>[{2019-10-12, Nos...</td><td>SP</td></tr>\n",
       "<tr><td>2018</td><td>02</td><td>60975174000363</td><td>355030</td><td>1929-07-01</td><td>3</td><td>0</td><td>000</td><td>0301060002</td><td>1</td><td>2018-02-06</td><td>2018-02-06</td><td>R54</td><td>null</td><td>null</td><td>null</td><td>M</td><td>355030</td><td>4</td><td>88</td><td>0</td><td>0</td><td>2080796</td><td>01</td><td>01</td><td>São Paulo</td><td>São Paulo</td><td>Sudeste</td><td>0.805</td><td>11253503</td><td>1521.101</td><td>Senilidade</td><td>null</td><td>{R54, null, null,...</td><td>{null, null, null...</td><td>false</td><td>6</td><td>[{2018-02-12, Car...</td><td>SP</td></tr>\n",
       "<tr><td>2019</td><td>10</td><td>60979457000707</td><td>353440</td><td>2017-10-10</td><td>1</td><td>0</td><td>000</td><td>0301070121</td><td>3</td><td>2019-10-11</td><td>2019-10-11</td><td>G808</td><td>null</td><td>null</td><td>null</td><td>M</td><td>353440</td><td>4</td><td>02</td><td>0</td><td>0</td><td>5493943</td><td>01</td><td>01</td><td>São Paulo</td><td>Osasco</td><td>Sudeste</td><td>0.776</td><td>666740</td><td>64.954</td><td>Outras formas de ...</td><td>null</td><td>{G808, null, null...</td><td>{null, null, null...</td><td>false</td><td>1</td><td>[{2019-10-12, Nos...</td><td>SP</td></tr>\n",
       "<tr><td>2018</td><td>02</td><td>60975174000363</td><td>355030</td><td>1960-09-17</td><td>3</td><td>0</td><td>000</td><td>0301060002</td><td>1</td><td>2018-02-06</td><td>2018-02-06</td><td>E149</td><td>null</td><td>null</td><td>null</td><td>M</td><td>355030</td><td>4</td><td>57</td><td>0</td><td>0</td><td>2080796</td><td>01</td><td>01</td><td>São Paulo</td><td>São Paulo</td><td>Sudeste</td><td>0.805</td><td>11253503</td><td>1521.101</td><td>Diabetes mellitus...</td><td>null</td><td>{E149, null, null...</td><td>{null, null, null...</td><td>false</td><td>6</td><td>[{2018-02-12, Car...</td><td>SP</td></tr>\n",
       "<tr><td>2019</td><td>10</td><td>60979457000707</td><td>353440</td><td>2018-06-08</td><td>1</td><td>0</td><td>000</td><td>0301070121</td><td>1</td><td>2019-10-11</td><td>2019-10-11</td><td>G510</td><td>null</td><td>null</td><td>null</td><td>M</td><td>353440</td><td>4</td><td>01</td><td>0</td><td>0</td><td>5493943</td><td>02</td><td>01</td><td>São Paulo</td><td>Osasco</td><td>Sudeste</td><td>0.776</td><td>666740</td><td>64.954</td><td>Paralisia de Bell</td><td>null</td><td>{G510, null, null...</td><td>{null, null, null...</td><td>false</td><td>1</td><td>[{2019-10-12, Nos...</td><td>SP</td></tr>\n",
       "<tr><td>2018</td><td>02</td><td>60975174000363</td><td>355030</td><td>1989-07-07</td><td>1</td><td>0</td><td>000</td><td>0301060002</td><td>1</td><td>2018-02-06</td><td>2018-02-06</td><td>H905</td><td>null</td><td>null</td><td>null</td><td>M</td><td>355030</td><td>4</td><td>28</td><td>0</td><td>0</td><td>2080796</td><td>01</td><td>01</td><td>São Paulo</td><td>São Paulo</td><td>Sudeste</td><td>0.805</td><td>11253503</td><td>1521.101</td><td>Perda de audição ...</td><td>null</td><td>{H905, null, null...</td><td>{null, null, null...</td><td>false</td><td>6</td><td>[{2018-02-12, Car...</td><td>SP</td></tr>\n",
       "<tr><td>2019</td><td>10</td><td>60979457000707</td><td>353440</td><td>2018-02-12</td><td>3</td><td>0</td><td>000</td><td>0301070121</td><td>1</td><td>2019-10-11</td><td>2019-10-11</td><td>G822</td><td>null</td><td>null</td><td>null</td><td>M</td><td>353440</td><td>4</td><td>01</td><td>0</td><td>0</td><td>5493943</td><td>02</td><td>01</td><td>São Paulo</td><td>Osasco</td><td>Sudeste</td><td>0.776</td><td>666740</td><td>64.954</td><td>Paraplegia não es...</td><td>null</td><td>{G822, null, null...</td><td>{null, null, null...</td><td>false</td><td>1</td><td>[{2019-10-12, Nos...</td><td>SP</td></tr>\n",
       "<tr><td>2018</td><td>02</td><td>60975174000363</td><td>355030</td><td>1999-11-12</td><td>1</td><td>0</td><td>000</td><td>0301060002</td><td>1</td><td>2018-02-06</td><td>2018-02-06</td><td>J350</td><td>null</td><td>null</td><td>null</td><td>M</td><td>355030</td><td>4</td><td>18</td><td>0</td><td>0</td><td>2080796</td><td>01</td><td>01</td><td>São Paulo</td><td>São Paulo</td><td>Sudeste</td><td>0.805</td><td>11253503</td><td>1521.101</td><td>Amigdalite crônica</td><td>null</td><td>{J350, null, null...</td><td>{null, null, null...</td><td>false</td><td>6</td><td>[{2018-02-12, Car...</td><td>SP</td></tr>\n",
       "<tr><td>2019</td><td>10</td><td>60979457000707</td><td>353440</td><td>2008-03-04</td><td>3</td><td>0</td><td>000</td><td>0301070121</td><td>1</td><td>2019-10-11</td><td>2019-10-11</td><td>R278</td><td>null</td><td>null</td><td>null</td><td>M</td><td>353440</td><td>4</td><td>11</td><td>0</td><td>0</td><td>5493943</td><td>01</td><td>01</td><td>São Paulo</td><td>Osasco</td><td>Sudeste</td><td>0.776</td><td>666740</td><td>64.954</td><td>Outros distúrbios...</td><td>null</td><td>{R278, null, null...</td><td>{null, null, null...</td><td>false</td><td>1</td><td>[{2019-10-12, Nos...</td><td>SP</td></tr>\n",
       "<tr><td>2018</td><td>02</td><td>60975174000363</td><td>355030</td><td>1981-10-01</td><td>3</td><td>0</td><td>000</td><td>0301060002</td><td>1</td><td>2018-02-06</td><td>2018-02-06</td><td>N390</td><td>null</td><td>null</td><td>null</td><td>M</td><td>355030</td><td>4</td><td>36</td><td>0</td><td>0</td><td>2080796</td><td>01</td><td>01</td><td>São Paulo</td><td>São Paulo</td><td>Sudeste</td><td>0.805</td><td>11253503</td><td>1521.101</td><td>Infecção do trato...</td><td>null</td><td>{N390, null, null...</td><td>{null, null, null...</td><td>false</td><td>6</td><td>[{2018-02-12, Car...</td><td>SP</td></tr>\n",
       "<tr><td>2019</td><td>10</td><td>60979457000707</td><td>353440</td><td>2018-10-02</td><td>3</td><td>0</td><td>000</td><td>0301070121</td><td>1</td><td>2019-10-11</td><td>2019-10-11</td><td>G822</td><td>null</td><td>null</td><td>null</td><td>M</td><td>353440</td><td>4</td><td>01</td><td>0</td><td>0</td><td>5493943</td><td>01</td><td>01</td><td>São Paulo</td><td>Osasco</td><td>Sudeste</td><td>0.776</td><td>666740</td><td>64.954</td><td>Paraplegia não es...</td><td>null</td><td>{G822, null, null...</td><td>{null, null, null...</td><td>false</td><td>1</td><td>[{2019-10-12, Nos...</td><td>SP</td></tr>\n",
       "<tr><td>2018</td><td>02</td><td>60975174000363</td><td>355030</td><td>1933-11-14</td><td>3</td><td>0</td><td>000</td><td>0301060002</td><td>1</td><td>2018-02-06</td><td>2018-02-06</td><td>I10</td><td>null</td><td>null</td><td>null</td><td>M</td><td>355030</td><td>4</td><td>84</td><td>0</td><td>0</td><td>2080796</td><td>01</td><td>01</td><td>São Paulo</td><td>São Paulo</td><td>Sudeste</td><td>0.805</td><td>11253503</td><td>1521.101</td><td>Hipertensão essen...</td><td>null</td><td>{I10, null, null,...</td><td>{null, null, null...</td><td>false</td><td>6</td><td>[{2018-02-12, Car...</td><td>SP</td></tr>\n",
       "<tr><td>2019</td><td>10</td><td>60979457000707</td><td>353440</td><td>1998-04-21</td><td>3</td><td>0</td><td>000</td><td>0302050019</td><td>1</td><td>2019-10-11</td><td>2019-10-11</td><td>G822</td><td>null</td><td>null</td><td>null</td><td>M</td><td>353440</td><td>4</td><td>21</td><td>0</td><td>0</td><td>5493943</td><td>01</td><td>01</td><td>São Paulo</td><td>Osasco</td><td>Sudeste</td><td>0.776</td><td>666740</td><td>64.954</td><td>Paraplegia não es...</td><td>null</td><td>{G822, null, null...</td><td>{null, null, null...</td><td>false</td><td>1</td><td>[{2019-10-12, Nos...</td><td>SP</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "DataFrame[ano_cmpt: string, mes_cmpt: string, cgc_hosp: string, munic_res: string, nasc: date, sexo: string, uti_mes_to: string, uti_int_to: string, proc_rea: string, qt_proc: string, dt_atend: date, dt_saida: date, diag_princ: string, diag_secun: string, cobranca: string, natureza: string, gestao: string, munic_mov: string, cod_idade: string, idade: string, dias_perm: string, morte: string, cnes: string, fonte: string, modalidade: string, nome_uf: string, nome_municipio: string, regiao: string, idhm: float, populacao_residente: int, area_unidade_territorial: float, diag_princ_desc: string, diag_secun_desc: string, diag_princ_detalhes: struct<sub_cat:string,classificacao:string,restr_sexo:string,causa_obito:string,descricao:string,desc_abrev:string,refer:string,excluidos:string>, diag_secun_detalhes: struct<sub_cat:string,classificacao:string,restr_sexo:string,causa_obito:string,descricao:string,desc_abrev:string,refer:string,excluidos:string>, feriado: string, distancia_feriado: int, feriado_info: array<struct<data:string,nome:string,tipo:string,descricao:string,uf:string,municipio:string,cod_municipio:string>>, sigla: string]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
